<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=7-w7HJ5EdeHiPPOlGrpNaMJQu3D9ZI-uZ8tPktq2JDs');ol{margin:0;padding:0}table td,table th{padding:0}.c17{-webkit-text-decoration-skip:none;color:#0000ff;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:10pt;font-family:"Garamond";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Garamond";font-style:normal}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Garamond";font-style:normal}.c10{padding-top:0pt;padding-bottom:3pt;line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c18{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Garamond";font-style:normal}.c12{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Garamond"}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c20{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:right}.c9{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-family:"Garamond";font-style:normal}.c19{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Garamond"}.c16{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c1{background-color:#ffffff;font-style:italic;color:#222222}.c22{background-color:#ffffff;max-width:451.3pt;padding:72pt 72pt 72pt 72pt}.c11{color:inherit;text-decoration:inherit}.c21{margin-left:36pt;text-indent:-36pt}.c14{width:33%;height:1px}.c7{background-color:#ffffff;color:#222222}.c8{height:10pt}.c6{font-style:italic}.c4{vertical-align:super}.c3{font-size:12pt}.c15{font-weight:700}.title{padding-top:0pt;color:#000000;font-size:14pt;padding-bottom:3pt;font-family:"Garamond";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:10pt;font-family:"Garamond"}p{margin:0;color:#000000;font-size:10pt;font-family:"Garamond"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Garamond";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Garamond";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Garamond";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Garamond";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Garamond";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Garamond";line-height:1.0;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c22 doc-content"><div><p class="c8 c20"><span class="c5"></span></p></div><p class="c10 title" id="h.gjdgxs"><span class="c18">Prettiness Algorithms: Smartphones and the Manipulation of Taste</span></p><p class="c12"><span class="c2">Leonardo Impett</span></p><p class="c12"><span class="c2">University of Cambridge</span></p><p class="c0 c8"><span class="c9 c3"></span></p><p class="c0 c8"><span class="c3 c9"></span></p><p class="c0"><span class="c3 c15">1. Neural Models of Visual Culture</span><sup class="c4 c3 c15"><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">In 2016, as a graduate student, I got involved in organising a conference called </span><span class="c6 c3">Ways of Machine Seeing</span><span class="c3">&nbsp;at Darwin College, Cambridge</span><sup class="c4 c3"><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span class="c6 c3">. </span><span class="c2">The conference&rsquo;s main idea - sadly not my own - was to encourage people to read across two major works on seeing. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">The first, John Berger&rsquo;s </span><span class="c6 c3">Ways of Seeing</span><span class="c3">&nbsp;(1972), is seminal text (and BBC documentary) on how vision is culturally and socially situated &ndash; a crucial problem for art history since the beginning of the last century</span><sup class="c4 c3"><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup><span class="c3">. Berger&rsquo;s focus was on the historical and socioeconomic relations surrounding pictures, both as they were originally made (e.g. displays of wealth in still-lives) and as they are seen now (e.g. how different economic classes go to galleries). A contemporary review praised his &ldquo;imaginative and often innovative use of Marxian method&rdquo;</span><sup class="c4 c3"><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup><span class="c3">; Berger built on previous Marxist theorists such as Walter Benjamin, and proposed radical new ideas of his own, including the term &ldquo;male gaze&rdquo;. The second, David Marr&rsquo;s </span><span class="c6 c3">Vision</span><span class="c2">&nbsp;(1982), lays out many of the fundamental assumptions of computer vision - partly through an analysis of human vision. The two texts are a several generations old; but both have survived through their central place in the pedagogy of their respective fields. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">What this juxtaposition heavily implied was the idea that computer vision models are themselves </span><span class="c6 c3">ways of seeing</span><span class="c3">: they contain just as much cultural, racial, social, and historical baggage as human vision &ndash; though undoubtedly encoded in a different way. This notion allows us to move on from the idea that computer vision or machine learning models are simply </span><span class="c6 c3">biased</span><span class="c3">: which is true, but unhelpfully implies the possibility of an unbiased model. An unbiased model would be theoretically possible but practically useless, since we are forced to expose computers to a particular cultural or historical viewpoint whenever we teach them what any man-made thing looks like. Algorithms, explains Louise Amoore, &ldquo;must necessarily discriminate to have any traction in the world&rdquo;</span><sup class="c4 c3"><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup><span class="c2">. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">To be clear, there is no doubt that algorithms (in computer vision and elsewhere) are often discriminatory. Algorithms used by U.S. courts to determine bail risks are hugely discriminatory against black defendants</span><sup class="c4 c3"><a href="#ftnt6" id="ftnt_ref6">[6]</a></sup><span class="c3">; commercial face detection systems perform 10-20% worse on darker female faces than lighter male faces</span><sup class="c4 c3"><a href="#ftnt7" id="ftnt_ref7">[7]</a></sup><span class="c3">; and object detection systems recognise everyday items like soap and foodstuffs more easily in the global north than in the global south</span><sup class="c4 c3"><a href="#ftnt8" id="ftnt_ref8">[8]</a></sup><span class="c3">. But </span><span class="c6 c3">bias</span><span class="c3">&nbsp;flattens a complex web of power relations (class, gender, geography) and their visual symptoms to a single percentage difference. Following Berger&rsquo;s Marxist insight, </span><span class="c3 c6">Ways of Machine Seeing</span><span class="c3">&nbsp;suggested another route; closer to literary scholar Ted Underwood&rsquo;s recent suggestion that &ldquo;[t]o understand why neural language models are dangerous (and fascinating), we need to approach them as models of culture&rdquo;</span><sup class="c4 c3"><a href="#ftnt9" id="ftnt_ref9">[9]</a></sup><span class="c2">. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">This approach would have been completely alien to me as a student of computer vision, where techniques either seemed to reflect intrinsic physical reality (in the case of, say, multiple view geometry) or universal features of human vision (e.g. in the similarity between sparse coding of natural images and primary visual cortex receptive fields). But I became involved in the </span><span class="c6 c3">Ways of Machine Seeing</span><span class="c3">&nbsp;conference partly because of a short research internship at Microsoft Research in Cairo a year earlier, where I worked on a computer vision problem which I will call </span><span class="c6 c3">prettiness estimation</span><span class="c2">. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">Why invent a new name for an existing field? Firstly, because computer scientists use a wide and inconsistent range of metaphors to describe the same task: aesthetic quality assessment, aesthetic image assessment, automatic visual aesthetics, photo aesthetic ranking, neural image assessment, aesthetic quality inference&hellip; And secondly, because the existing terms are often misleading. Most refer to &ldquo;aesthetics&rdquo; &ndash; but </span><span class="c6 c3">prettiness estimation</span><span class="c3">&nbsp;has very little to do with the rich intellectual history of aesthetics. Scientists ask a set of human annotators for a rating of a digital photo out of 5 stars</span><sup class="c4 c3"><a href="#ftnt10" id="ftnt_ref10">[10]</a></sup><span class="c3">, or &ldquo;how beautiful is this picture&rdquo; (a sliding scale from 1 to 5, where 4 is &ldquo;Professional&rdquo;)</span><sup class="c4 c3"><a href="#ftnt11" id="ftnt_ref11">[11]</a></sup><span class="c3">; or simply their opinion on &ldquo;photo quality&rdquo;</span><sup class="c4 c3"><a href="#ftnt12" id="ftnt_ref12">[12]</a></sup><span class="c3">. Others use amateur photography websites where online users rank each other&rsquo;s photographs, including Photo.net</span><sup class="c4 c3"><a href="#ftnt13" id="ftnt_ref13">[13]</a></sup><span class="c3">&nbsp;and DPChallenge.com (a 2012 snapshot of which forms the Aesthetic Visual Analysis dataset, or AVA, perhaps the most widely-used benchmark)</span><sup class="c4 c3"><a href="#ftnt14" id="ftnt_ref14">[14]</a></sup><span class="c2">. &ldquo;How pretty is this photo?&rdquo; is what they&rsquo;re really asking &ndash; hence my proposed formulation. &nbsp;</span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c9 c3">2. Whose prettiness?</span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">On hearing of the existence of this family of algorithms in computer science, colleagues in the humanities most commonly raise two objections: either that the problem is useless (why would anybody need such an algorithm?), and that it is impossible. The first is the easiest to dismiss. Imagine you have a folder of images on your computer or smartphone, or perhaps a large set of photographs taken at a wedding, or during a holiday. Modern user interfaces often display a &lsquo;preview image&rsquo; for such a folder or event; so we have to automatically choose a single image to represent the whole set. Perhaps some photographs are blurry, and others might be &lsquo;pocket-dials&rsquo;, taken by accident. We should at least be able to eliminate the </span><span class="c6 c3">least pretty</span><span class="c2">&nbsp;images, and thus choose a reasonable photograph. As we shall see, this is not the only use of prettiness estimators - but it was the use-case we were considering back in 2015. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">What about the second objection: how could a machine possibly reproduce deeply subjective judgements around beauty? We might assume everyone involved in creating these datasets has a deeply individual set of preferences; this turns out not to be the case. On the DPChallenge website, photo rating scores go from 1 to 10; the average standard deviation of scores for individual photographs is less than 1.4</span><sup class="c4 c3"><a href="#ftnt15" id="ftnt_ref15">[15]</a></sup><span class="c3">. The authors of the Aesthetics and Attributes Database (AADB) find that &ldquo;98.45% batches have significant agreement among raters&rdquo; - and that therefore &ldquo;the annotations are reliable for scientific research&rdquo;</span><sup class="c4 c3"><a href="#ftnt16" id="ftnt_ref16">[16]</a></sup><span class="c2">. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">Opinions on photo prettiness are consistent enough in these datasets, then, but are they reproducible algorithmically? Overall performance on the task is constantly improving, like any other in computer vision - but a 2019 model trained on DPChallenge scores gives a Pearson linear correlation coefficient (a measure of how well predictions agree with average user scores, where 1.0 is the highest) of 0.756</span><sup class="c4 c3"><a href="#ftnt17" id="ftnt_ref17">[17]</a></sup><span class="c3">. To put this into context, the average individual user rating has a Pearson correlation of roughly 0.45 with the overall average</span><sup class="c4 c3"><a href="#ftnt18" id="ftnt_ref18">[18]</a></sup><span class="c3">. This leads to the paradoxical conclusion that machines have reached &lsquo;superhuman&rsquo; performance on an intrinsically subjective task: in the sense that algorithms are able to reproduce the average prettiness of an image far more reliably than we seem to</span><sup class="c4 c3"><a href="#ftnt19" id="ftnt_ref19">[19]</a></sup><span class="c2">. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">Not only are prettiness scores in these datasets surprisingly consistent and reproducible; it appears that even their inconsistencies are predictable. In a 2016 paper</span><sup class="c4 c3"><a href="#ftnt20" id="ftnt_ref20">[20]</a></sup><span class="c2">, former colleagues of mine from the Image and Visual Representation Lab in Lausanne describe an algorithm that not only accurately predicts the average aesthetic score of DPChallenge images, but also the shape of the histogram of scores. Their model can differentiate, in other words, between images whose prettiness (or otherwise) is generally accepted, those rarer images that invoke some kind of controversy or difference of opinion. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">We might suppose that the fact that these algorithms are able to crack prettiness so convincingly points to the predictability of our own taste in images. Our own taste - but who are &lsquo;we&rsquo;? Clearly not all eight billion or so humans alive today, as one of the first prettiness estimation papers admitted in 2006: &ldquo;Ideally, the data should have been collected from a random sample of human subjects under controlled setup, but resource constraints prevented us from doing so&rdquo;</span><sup class="c4 c3"><a href="#ftnt21" id="ftnt_ref21">[21]</a></sup><span class="c2">. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c2">It turns out that we can point to a &lsquo;we&rsquo;. Machine vision datasets are often &lsquo;crowdsourced&rsquo; through platforms like Amazon Mechanical Turk; making it difficult to understand whose judgements are being captured. Not so for the the most commonly-used dataset for prettiness estimation, the online photography competition website DPChallenge - which allows members to give their location, biographical summary, age, and even a list of cameras owned. And unlike most computer vision datasets, the selection of users in DPChallenge operates on two levels: since its users are both creating and scoring the images. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">At time of writing</span><sup class="c4 c3"><a href="#ftnt22" id="ftnt_ref22">[22]</a></sup><span class="c2">, of the 10 users who have received the most votes on DPChallenge, 6 list their location as within the US (one each in Pennsylvania, Wisconsin, California, Arizona, New Jersey, Massachusetts); 2 as Canada; and 2 the UK. Eight give their age: all between 53 and 75. Five list iPhones alongside their digital cameras. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">This is in no way a criticism of the DPChallenge community - it is clearly not </span><span class="c6 c3">intended</span><span class="c3">&nbsp;to be a representative cross-section of the global population. Users of DPChallenge are naturally more likely to have the time and disposable income to pursue the hobby of digital photography. &ldquo;Prosumers&rdquo;</span><sup class="c4 c3"><a href="#ftnt23" id="ftnt_ref23">[23]</a></sup><span class="c2">, the paper presenting AVA (the dataset based on DPChallenge) calls them, in the sense that they create and consume content; but this is also the industry&rsquo;s name for the market segment of the most expensive amateur cameras. We don&rsquo;t need to be die-hard Bourdieuans to suggest that DPChallenge might constitute a social field, which attracts participants selectively (along geographical, economic, racial, professional, class lines), has them compete for forms of cultural capital (peer voting, winning challenges), and shapes their tastes. We might also hypothesise that the apparent predictability of the dataset&rsquo;s taste is, at least in part, down to the preselection of agents (&ldquo;sample bias&rdquo;) and the convergent dynamics of this field. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">If the taste-system DPChallenge is so predictable, what are its distinguishing characteristics? Challenge-winning photographs</span><sup class="c4 c3"><a href="#ftnt24" id="ftnt_ref24">[24]</a></sup><span class="c3">&nbsp;often feature extremes of colour: either dramatically-coloured skies, captured in high-dynamic-range (e.g. over Copenhagen</span><sup class="c4 c3"><a href="#ftnt25" id="ftnt_ref25">[25]</a></sup><span class="c3">; an Icelandic mountain</span><sup class="c4 c3"><a href="#ftnt26" id="ftnt_ref26">[26]</a></sup><span class="c3">; or Dutch windmills</span><sup class="c4 c3"><a href="#ftnt27" id="ftnt_ref27">[27]</a></sup><span class="c3">) or in black-and-white (of telephones</span><sup class="c4 c3"><a href="#ftnt28" id="ftnt_ref28">[28]</a></sup><span class="c3">, stairs</span><sup class="c4 c3"><a href="#ftnt29" id="ftnt_ref29">[29]</a></sup><span class="c3">, footpaths</span><sup class="c4 c3"><a href="#ftnt30" id="ftnt_ref30">[30]</a></sup><span class="c2">). A very large number are landscapes or &lsquo;still lifes&rsquo;. They frequently include domestic or wild animals; only rarely do they include people. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">If we take seriously the proposal that trained neural networks are models of dataset culture, one way to see the visual logic of DPChallenge through the lens of an algorithm that&rsquo;s been trained on it. Lu et al, in presenting a new model trained on images from both DPChallenge and Photo.net</span><sup class="c4 c3"><a href="#ftnt31" id="ftnt_ref31">[31]</a></sup><span class="c3">, show the 15 images in the dataset that their algorithm ranks as prettiest. 13 are landscapes; the other 2 are still lifes. All are either monochrome or highly saturated, and none show any people. We might hypothesise that, in this visual logic, Komar and Melamid&rsquo;s 1995 landscape work </span><span class="c6 c3">USA&rsquo;s Most Wanted Painting</span><span class="c2">&nbsp;would do rather well. Its conditions of production are somewhat analogous: the artists commissioned a market research firm to gather data on customer preferences about colour, size, iconography etc, and designed a painting based on the survey outcomes. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0 c8"><span class="c9 c3"></span></p><p class="c0"><span class="c9 c3">3. Enhance!</span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">Why is it important to understand - or at least to highlight - the cultural situatedness of prettiness estimation datasets and networks? Because their use, it turns out, goes far beyond preview image selection. Several research papers have already suggested incorporating aesthetic scores into image search ranking algorithms</span><sup class="c4 c3"><a href="#ftnt32" id="ftnt_ref32">[32]</a></sup><span class="c4 c3">,</span><sup class="c4 c3"><a href="#ftnt33" id="ftnt_ref33">[33]</a></sup><span class="c3">. Others suggest using prettiness estimators as the basis for automatic image cropping (e.g. where a square preview of a rectangular image must be generated): keeping only the &lsquo;best&rsquo; part of the image</span><sup class="c4 c3"><a href="#ftnt34" id="ftnt_ref34">[34]</a></sup><span class="c4 c3">,</span><sup class="c4 c3"><a href="#ftnt35" id="ftnt_ref35">[35]</a></sup><span class="c4 c3">,</span><sup class="c4 c3"><a href="#ftnt36" id="ftnt_ref36">[36]</a></sup><span class="c3">. Although its model was trained on saliency rather than prettiness, Twitter&rsquo;s image-cropping algorithm generated controversy in October 2020 when it was shown empirically to favour the inclusion of white people over black people</span><sup class="c4 c3"><a href="#ftnt37" id="ftnt_ref37">[37]</a></sup><span class="c2">. Through various mechanisms, then, prettiness estimation algorithms have the potential to severely influence visibility and invisibility in digital visual culture. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">Through a similar logic, other aspects of the image can be manipulated in order to increase the measured prettiness of a digital image: its colour, saturation, brightness levels, and so on. Automatically enhancing images with computer vision has become one of the principal weapons in the smartphone camera arms-race of the past decade - and though there are various techniques, almost all require a training dataset of &lsquo;good&rsquo; images (i.e. something like DPChallenge). Apple&rsquo;s algorithm, </span><span class="c6 c3">Deep Fusion</span><span class="c3">, was marketed as a major feature of the new iPhone 11 in 2019. Google&rsquo;s system is instead part of Google Photos. Although we don&rsquo;t know which dataset the Google algorithm is trained on</span><sup class="c4 c3"><a href="#ftnt38" id="ftnt_ref38">[38]</a></sup><span class="c3">, its enhancements</span><sup class="c4 c3"><a href="#ftnt39" id="ftnt_ref39">[39]</a></sup><span class="c3">&nbsp;follow the visual logic of DPChallenge: highly-saturated HDR images or monochrome geometricism. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">Neural image enhancement fundamentally changes the relationship between public visual taste and neural models thereof. Once neural image enhancement algorithms are an everyday part of smartphone photography, they start to play a role in </span><span class="c6 c3">defining</span><span class="c3">&nbsp;taste. Neural networks for prettiness estimation, therefore, are influencing the phenomena they aim to model. Before the invention of the World Wide Web, sociologist Anthony Giddens described the </span><span class="c6 c3">double hermeneutic</span><sup class="c4 c3"><a href="#ftnt40" id="ftnt_ref40">[40]</a></sup><span class="c2">, in which sociological models affect the behaviour of the people they describe. Prettiness estimation algorithms in image enhancement, auto-cropping, and search engines form a double hermeneutic: between neural models of human behaviour and the behaviour itself. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">This is a crucial difference between the cultural biases in relatively &lsquo;objective&rsquo; tasks (object detection, person recognition, etc), and those in highly personal tasks like prettiness estimation. At the start of this paper, we saw a study on how Facebook&rsquo;s object detection algorithm performs significantly worse on images of everyday objects taken in low-income countries;</span><span class="c4 c3">&nbsp;</span><sup class="c3 c4"><a href="#ftnt41" id="ftnt_ref41">[41]</a></sup><span class="c2">&nbsp;presumably because it had been trained on images from higher-income countries. As potentially problematic as this is, there is no suggestion this will lead to changes in the phenomenon being modelled. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c2">We have a feedback loop, then, between public taste in photographs and the neural models intended to model it. As new datasets for prettiness estimation are created, newer tendencies in taste are incorporated in the neural models; whilst some &lsquo;online&rsquo; machine learning systems might be updating their behaviour in real time based on changing user behaviour (i.e. changing taste in images). The loop is biased in at least two ways: firstly, because the initial models created to model the phenomenon are based on very particular subsections of the global population (whether or not they actually use DPChallenge); and secondly, because future datasets will necessarily be similarly skewed towards photographers (smartphone or digital camera owners) and internet users (i.e. those generating quantitative training data on prettiness, possibly unwittingly through the use of social media platforms, search engines etc). </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">What do we know about the dynamics of this feedback loop? We know that it necessarily has the tendency to homogenise visual taste - and that its centre of gravity is the taste-system we have explored above. Images get made to look like </span><span class="c6 c3">other pretty pictures</span><span class="c3">&nbsp;- this is the logic of pattern recognition. We don&rsquo;t know how strong the feedback loop is, and it may be that global, distributed visual taste is barely influenced by the enhancements made by smartphones. But the contemporary importance of smartphones as tools of image-creation (compared, say, to pocket digital cameras); the ubiquity of image enhancement software in newer models; and the association of particular algorithms (Apple&rsquo;s </span><span class="c6 c3">Deep Fusion</span><span class="c2">) with expensive hardware (iPhones) might make us suspect otherwise. Because enhancements are often performed silently and automatically at the time of capture, image enhancement algorithms rope us in as collaborators: their pretty pictures are our pretty pictures. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0"><span class="c3">As anyone who has placed a microphone near a loudspeaker knows, feedback loops are not always stable. They can implode just as easily as they explode. There are no direct historical precedents to global cultural feedback systems of this kind &ndash; between a distributed system of visual taste, and a centralised algorithm trying to model and predict it. However, many of the the basic methods of image enhancement used today (the manipulation of colour palettes, false depth of field, artificial sharpening) are not so far removed from the colour-filters which made Instagram successful in the early 2010s. Their commercial propositions are in a sense similar: to make images from smartphones look like they came from &ldquo;real&rdquo; cameras (in Instagram&rsquo;s case, film cameras; in contemporary photo augmentation, DSLRs). Instagram&rsquo;s homogenisation of digital visual culture even led to changes in commercial architecture</span><sup class="c4 c3"><a href="#ftnt42" id="ftnt_ref42">[42]</a></sup><span class="c2">, but it also brought the &lsquo;#nofilter&rsquo; trend of 2014 (a contradictory, and only very partial, rejection of Instagram&rsquo;s algorithmic aesthetic). Instagram&rsquo;s filter-based algorithms were much simpler, and it had no &lsquo;online learning&rsquo; feedback loop between data and model; but it is, perhaps, an indication that homogenising algorithms in online visual culture have the potential (even the tendency) to collapse in on themselves. </span></p><p class="c0 c8"><span class="c2"></span></p><p class="c0 c8"><span class="c2"></span></p><div><p class="c0 c8"><span class="c5"></span></p></div><hr class="c14"><div><p class="c0 c21"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c5">&nbsp;This is an edited version of an article which appeared in </span><span class="c13 c6">EthicAI=Artificial Intelligence &amp; Ethics</span><span class="c5">, republished by kind permission of the Goethe-Institute Bulgaria. </span></p></div><div><p class="c0"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c5">&nbsp;The conference was organised by Alan Blackwell and Anne Alexander; it has since spawned various other events, networks, and publications, including a recent special issue: Azar, Mitra, Geoff Cox, and Leonardo Impett. &ldquo;Introduction: ways of machine seeing.&rdquo;&nbsp;</span><span class="c13 c6">AI &amp; SOCIETY</span><span class="c5">&nbsp;(2021): 1-12. </span></p></div><div><p class="c0"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c5">&nbsp;E.g. Heinrich W&ouml;lfflin&rsquo;s &lsquo;History of Seeing&rsquo;; see Davis, Whitney. &quot;Succession and Recursion in Heinrich W&ouml;lfflin&#39;s Principles of Art History.&quot;&nbsp;</span><span class="c6 c13">The Journal of Aesthetics and Art Criticism</span><span class="c5">&nbsp;73, no. 2 (2015): 157-164.</span></p></div><div><p class="c0"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span class="c5">&nbsp;Wallach, Alan. &ldquo;John Berger, Ways of Seeing&rdquo; (book review), in </span><span class="c13 c6">Artforum</span><span class="c5">&nbsp;February 1976: 44-45. &nbsp;</span></p></div><div><p class="c0"><a href="#ftnt_ref5" id="ftnt5">[5]</a><span class="c5">&nbsp;Amoore, Louise.&nbsp;</span><span class="c13 c6">Cloud ethics</span><span class="c5">. Duke University Press, 2020: 8</span></p></div><div><p class="c0"><a href="#ftnt_ref6" id="ftnt6">[6]</a><span class="c5">&nbsp;Angwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. &ldquo;Machine bias.&rdquo;&nbsp;</span><span class="c13 c6">ProPublica, </span><span class="c5">May&nbsp;23 (2016): 139-159.</span></p></div><div><p class="c0"><a href="#ftnt_ref7" id="ftnt7">[7]</a><span class="c5">&nbsp;Buolamwini, Joy, and Timnit Gebru. &quot;Gender shades: Intersectional accuracy disparities in commercial gender classification.&quot; In&nbsp;</span><span class="c13 c6">Conference on fairness, accountability and transparency</span><span class="c5">, pp. 77-91. PMLR, 2018.</span></p></div><div><p class="c0"><a href="#ftnt_ref8" id="ftnt8">[8]</a><span class="c5">&nbsp;De Vries, Terrance, Ishan Misra, Changhan Wang, and Laurens Van der Maaten. &quot;Does object recognition work for everyone?.&quot; In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 52-59. 2019.</span></p></div><div><p class="c0"><a href="#ftnt_ref9" id="ftnt9">[9]</a><span>&nbsp;Underwood, Ted. </span><span class="c6">Mapping the latent spaces of culture. </span><span>2021 </span><span class="c16"><a class="c11" href="https://www.google.com/url?q=http://dx.doi.org/10.17613/faaa-1r21&amp;sa=D&amp;source=editors&amp;ust=1678056002945334&amp;usg=AOvVaw0bV0LVquT1ina82dajbgIO">http://dx.doi.org/10.17613/faaa-1r21</a></span><span class="c5">&nbsp;</span></p></div><div><p class="c0"><a href="#ftnt_ref10" id="ftnt10">[10]</a><span>&nbsp;</span><span class="c7">Kong, Shu, Xiaohui Shen, Zhe Lin, Radomir Mech, and Charless Fowlkes. &quot;Photo aesthetics ranking network with attributes and content adaptation.&quot; In </span><span class="c1">European conference on computer vision</span><span class="c7">, pp. 662-679. Springer, Cham, 2016.</span></p></div><div><p class="c0"><a href="#ftnt_ref11" id="ftnt11">[11]</a><span>&nbsp;</span><span class="c7">Schifanella, Rossano, Miriam Redi, and Luca Maria Aiello. &quot;An image is worth more than a thousand favorites: Surfacing the hidden beauty of flickr pictures.&quot; In </span><span class="c1">Proceedings of the International AAAI Conference on Web and Social Media</span><span class="c7">, vol. 9, no. 1, pp. 397-406. 2015.</span></p></div><div><p class="c0"><a href="#ftnt_ref12" id="ftnt12">[12]</a><span>&nbsp;</span><span class="c7">Luo, Wei, Xiaogang Wang, and Xiaoou Tang. &quot;Content-based photo quality assessment.&quot; In </span><span class="c1">2011 International Conference on Computer Vision</span><span class="c7">, pp. 2206-2213. IEEE, 2011.</span></p></div><div><p class="c0"><a href="#ftnt_ref13" id="ftnt13">[13]</a><span>&nbsp;</span><span class="c7">Datta, Ritendra, Dhiraj Joshi, Jia Li, and James Z. Wang. &quot;Studying aesthetics in photographic images using a computational approach.&quot; In </span><span class="c1">European Conference on Computer Vision</span><span class="c7">, pp. 288-301. Springer, Berlin, Heidelberg, 2006.</span></p></div><div><p class="c0"><a href="#ftnt_ref14" id="ftnt14">[14]</a><span>&nbsp;</span><span class="c7">Murray, Naila, Luca Marchesotti, and Florent Perronnin. &quot;AVA: A large-scale database for aesthetic visual analysis.&quot; In </span><span class="c1">2012 IEEE Conference on Computer Vision and Pattern Recognition</span><span class="c7">, pp. 2408-2415. IEEE, 2012.</span></p></div><div><p class="c0"><a href="#ftnt_ref15" id="ftnt15">[15]</a><span>&nbsp;Kim, Won-Hee, Jun-Ho Choi, and Jong-Seok Lee. &quot;Subjectivity in aesthetic quality assessment of digital photographs: Analysis of user comments.&quot; In </span><span class="c6">Proceedings of the 23rd ACM international conference on Multimedia</span><span class="c5">, pp. 983-986. 2015.</span></p></div><div><p class="c0"><a href="#ftnt_ref16" id="ftnt16">[16]</a><span>&nbsp;</span><span class="c7">Kong, Shu, Xiaohui Shen, Zhe Lin, Radomir Mech, and Charless Fowlkes. &quot;Photo aesthetics ranking network with attributes and content adaptation.&quot; In </span><span class="c1">European Conference on Computer Vision</span><span class="c7">, pp. 662-679. Springer, Cham, 2016.</span></p></div><div><p class="c0"><a href="#ftnt_ref17" id="ftnt17">[17]</a><span>&nbsp;Hosu, Vlad, Bastian Goldlucke, and Dietmar Saupe. &quot;Effective aesthetics prediction with multi-level spatially pooled features.&quot; In </span><span class="c6">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="c5">, pp. 9375-9383. 2019.</span></p></div><div><p class="c0"><a href="#ftnt_ref18" id="ftnt18">[18]</a><span>&nbsp;Code by the author available at </span><span class="c16"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.6084/m9.figshare.19196321&amp;sa=D&amp;source=editors&amp;ust=1678056002947046&amp;usg=AOvVaw3a8gQbCoaf5ORnwC2t0H_P">doi.org/10.6084/m9.figshare.19196321</a></span><span class="c5">&nbsp;; note that the code makes a small simplification in measuring 1-versus-all rather than 1-versus-rest, given the large number of scores. </span></p></div><div><p class="c0"><a href="#ftnt_ref19" id="ftnt19">[19]</a><span class="c5">&nbsp;Although this is not quite a fair test, since in prettiness estimation datasets individual users are generally being asked to transcribe their own opinion of each image, rather than their estimate of an overall average opinion. </span></p></div><div><p class="c0"><a href="#ftnt_ref20" id="ftnt20">[20]</a><span>&nbsp;Jin, B., Segovia, M.V.O. and S&uuml;sstrunk, S., 2016, September. Image aesthetic predictors based on weighted CNNs. In 2016 </span><span class="c6">IEEE International Conference on Image Processing (ICIP)</span><span class="c5">&nbsp;(pp. 2291-2295). Ieee.</span></p></div><div><p class="c0"><a href="#ftnt_ref21" id="ftnt21">[21]</a><span>&nbsp;</span><span class="c7">Datta, Ritendra, Dhiraj Joshi, Jia Li, and James Z. Wang. &quot;Studying aesthetics in photographic images using a computational approach.&quot; In </span><span class="c1">European conference on computer vision</span><span class="c7">, pp. 288-301. Springer, Berlin, Heidelberg, 2006.</span></p></div><div><p class="c0"><a href="#ftnt_ref22" id="ftnt22">[22]</a><span>&nbsp;</span><span class="c16"><a class="c11" href="https://www.google.com/url?q=https://www.dpchallenge.com/top_10.php?view%3Dmost_votes_given&amp;sa=D&amp;source=editors&amp;ust=1678056002947831&amp;usg=AOvVaw0QnA7qWMzT5ROaYXNXOV5-">https://www.dpchallenge.com/top_10.php?view=most_votes_given</a></span><span>&nbsp;- note that the AVA dataset is a &ldquo;fixed&rdquo; snapshot taken before 2012. A snapshot which is roughly contemporary is given at </span><span class="c16"><a class="c11" href="https://www.google.com/url?q=https://web.archive.org/web/20120518214949/https://www.dpchallenge.com/top_10.php?view%3Dmost_votes_given&amp;sa=D&amp;source=editors&amp;ust=1678056002948077&amp;usg=AOvVaw2Bcm_3nWQagsBUOFGUVE7M">https://web.archive.org/web/20120518214949/https://www.dpchallenge.com/top_10.php?view=most_votes_given</a></span><span class="c5">&nbsp;(in this list, one of the top 10 is based in South Africa). I don&rsquo;t want to give the impression that users are only from North America or the UK &ndash; a surprising number turn out to be based in Iceland. </span></p></div><div><p class="c0"><a href="#ftnt_ref23" id="ftnt23">[23]</a><span>&nbsp;</span><span class="c7">Murray, Naila, Luca Marchesotti, and Florent Perronnin. &quot;AVA: A large-scale database for aesthetic visual analysis.&quot; In </span><span class="c1">2012 IEEE Conference on Computer Vision and Pattern Recognition</span><span class="c7">, pp. 2408-2415. IEEE, 2012.</span></p></div><div><p class="c0"><a href="#ftnt_ref24" id="ftnt24">[24]</a><span class="c5">&nbsp;All previous challenge-winning photographs are visible at https://www.dpchallenge.com/challenge_archive.php</span></p></div><div><p class="c0"><a href="#ftnt_ref25" id="ftnt25">[25]</a><span class="c5">&nbsp;https://www.dpchallenge.com/image.php?IMAGE_ID=923702</span></p></div><div><p class="c0"><a href="#ftnt_ref26" id="ftnt26">[26]</a><span class="c5">&nbsp;https://www.dpchallenge.com/image.php?IMAGE_ID=933011</span></p></div><div><p class="c0"><a href="#ftnt_ref27" id="ftnt27">[27]</a><span class="c5">&nbsp;https://www.dpchallenge.com/image.php?IMAGE_ID=928097</span></p></div><div><p class="c0"><a href="#ftnt_ref28" id="ftnt28">[28]</a><span class="c5">&nbsp;https://www.dpchallenge.com/image.php?IMAGE_ID=922769 </span></p></div><div><p class="c0"><a href="#ftnt_ref29" id="ftnt29">[29]</a><span class="c5">&nbsp;https://www.dpchallenge.com/image.php?IMAGE_ID=921577</span></p></div><div><p class="c0"><a href="#ftnt_ref30" id="ftnt30">[30]</a><span class="c5">&nbsp;https://www.dpchallenge.com/image.php?IMAGE_ID=919866</span></p></div><div><p class="c0"><a href="#ftnt_ref31" id="ftnt31">[31]</a><span>&nbsp;Lu, Xin, Zhe Lin, Hailin Jin, Jianchao Yang, and James Z. Wang. &quot;Rating image aesthetics using deep learning.&quot; </span><span class="c6">IEEE Transactions on Multimedia</span><span class="c5">&nbsp;17, no. 11 (2015): 2021-2034.</span></p></div><div><p class="c0"><a href="#ftnt_ref32" id="ftnt32">[32]</a><span>&nbsp;San Pedro, Jose, Tom Yeh, and Nuria Oliver. &ldquo;Leveraging user comments for aesthetic aware image search reranking.&rdquo; In </span><span class="c6">Proceedings of the 21st international conference on World Wide Web</span><span class="c5">, pp. 439-448. 2012.</span></p></div><div><p class="c0"><a href="#ftnt_ref33" id="ftnt33">[33]</a><span>&nbsp;Redi, Miriam, Frank Z. Liu, and Neil O&#39;Hare. &ldquo;Bridging the aesthetic gap: The wild beauty of web imagery.&rdquo; In </span><span class="c6">Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval</span><span class="c5">, pp. 242-250. 2017.</span></p></div><div><p class="c0"><a href="#ftnt_ref34" id="ftnt34">[34]</a><span>&nbsp;Yan, Jianzhou, Stephen Lin, Sing Bing Kang, and Xiaoou Tang. &ldquo;Learning the change for automatic image cropping.&rdquo; In </span><span class="c6">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="c5">, pp. 971-978. 2013.</span></p></div><div><p class="c0"><a href="#ftnt_ref35" id="ftnt35">[35]</a><span>&nbsp;Kao, Yueying, Ran He, and Kaiqi Huang. &ldquo;Automatic image cropping with aesthetic map and gradient energy map.&rdquo; In </span><span class="c6">2017 IEEE International Conference on Acoustics, Speech and Ssignal Processing (ICASSP),</span><span class="c5">&nbsp;pp. 1982-1986. IEEE, 2017.</span></p></div><div><p class="c0"><a href="#ftnt_ref36" id="ftnt36">[36]</a><span>&nbsp;Wang, Wenguan, Jianbing Shen, and Haibin Ling. &ldquo;A deep network solution for attention and aesthetics aware photo cropping.&rdquo; </span><span class="c6">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="c5">&nbsp;41, no. 7 (2018): 1531-1544.</span></p></div><div><p class="c0"><a href="#ftnt_ref37" id="ftnt37">[37]</a><span>&nbsp;Yee, Kyra, Uthaipon Tantipongpipat, and Shubhanshu Mishra. &ldquo;Image cropping on twitter: Fairness metrics, their limitations, and the importance of representation, design, and agency.&rdquo; </span><span class="c6">Proceedings of the ACM on Human-Computer Interaction</span><span class="c5">&nbsp;5, no. CSCW2 (2021): 1-24.</span></p></div><div><p class="c0"><a href="#ftnt_ref38" id="ftnt38">[38]</a><span>&nbsp;One research paper from Google (an experiment with Street View) explicitly chose not to use DPChallenge, instead working with images from professional photographers directly - see: Fang, Hui, and Meng Zhang. &ldquo;Creatism: A deep-learning photographer capable of creating professional work.&rdquo;</span><span class="c6">&nbsp;arXiv preprint</span><span class="c5">&nbsp;arXiv:1707.03491 (2017).</span></p></div><div><p class="c0"><a href="#ftnt_ref39" id="ftnt39">[39]</a><span class="c5">&nbsp;As its enhancements are often to do with the saturation or desaturation of colour, example images are not suitable for a black-and-white publication; instead, two example enhancements by the author are available at </span><span class="c17"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.6084/m9.figshare.19336634&amp;sa=D&amp;source=editors&amp;ust=1678056002950116&amp;usg=AOvVaw3OQI2cnETWdAKdrxpX9NCu">doi.org/10.6084/m9.figshare.19336634</a></span></p></div><div><p class="c0"><a href="#ftnt_ref40" id="ftnt40">[40]</a><span>&nbsp;Giddens, Anthony. </span><span class="c6">Social theory and modern sociology</span><span class="c5">. Stanford University Press, 1987. The &lsquo;philosopher of New Labour&rsquo;, Giddens shares little politically with Berger; but the sociological phenomenon he identifies is of relevance nonetheless &nbsp;</span></p></div><div><p class="c0"><a href="#ftnt_ref41" id="ftnt41">[41]</a><span>&nbsp;De Vries, Terrance, Ishan Misra, Changhan Wang, and Laurens Van der Maaten. &ldquo;Does object recognition work for everyone?.&rdquo; In </span><span class="c6">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</span><span class="c5">, pp. 52-59. 2019.</span></p></div><div><p class="c0"><a href="#ftnt_ref42" id="ftnt42">[42]</a><span class="c5">&nbsp;Newton, Casey. &ldquo;Instagram is Pushing Restaurants to be Kitschy, Colorful, and Irresistable to Photographers&rdquo;. </span><span class="c13 c6">The Verge</span><span class="c5">, July 2020 </span><span class="c17"><a class="c11" href="https://www.google.com/url?q=https://www.theverge.com/2017/7/20/16000552/instagram-restaurant-interior-design-photo-friendly-media-noche&amp;sa=D&amp;source=editors&amp;ust=1678056002950841&amp;usg=AOvVaw3CCvRdP6sZt2UovUnNx0Ux">https://www.theverge.com/2017/7/20/16000552/instagram-restaurant-interior-design-photo-friendly-media-noche</a></span><span class="c5">&nbsp;</span></p></div></body></html>